---
title: 'Data Factory-Tutorial: erste Datenpipeline | Microsoft Docs'
description: In diesem Azure Data Factory-Tutorial erfahren Sie, wie Sie eine Data Factory erstellen und planen, die Daten unter Verwendung eines Hive-Skripts in einem Hadoop-Cluster verarbeitet.
services: data-factory
keywords: Azure Data Factory-Tutorial, Hadoop-Cluster, Hadoop-Hive
documentationcenter: 
author: spelluru
manager: jhubbard
editor: 
ms.assetid: 81f36c76-6e78-4d93-a3f2-0317b413f1d0
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 07/10/2017
ms.author: spelluru
ms.translationtype: HT
ms.sourcegitcommit: 646886ad82d47162a62835e343fcaa7dadfaa311
ms.openlocfilehash: 08e2988d455cca21726162d9fb128e91fd51f463
ms.contentlocale: de-de
ms.lasthandoff: 08/24/2017

---
# <a name="tutorial-build-your-first-pipeline-to-transform-data-using-hadoop-cluster"></a>Tutorial: Erstellen Ihrer ersten Pipeline zur Transformierung von Daten mithilfe eines Hadoop-Clusters
> [!div class="op_single_selector"]
> * [Übersicht und Voraussetzungen](data-factory-build-your-first-pipeline.md)
> * [Azure-Portal](data-factory-build-your-first-pipeline-using-editor.md)
> * [Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)
> * [PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)
> * [Resource Manager: Vorlage](data-factory-build-your-first-pipeline-using-arm.md)
> * [REST-API](data-factory-build-your-first-pipeline-using-rest-api.md)

In diesem Tutorial erstellen Sie Ihre erste Azure Data Factory mit einer Datenpipeline. Die Pipeline transformiert Eingabedaten durch Ausführen des Hive-Skripts in einem Azure HDInsight (Hadoop)-Cluster in Ausgabedaten.  

Dieser Artikel enthält eine Übersicht und Voraussetzungen für das Tutorial. Nach dem Erfüllen der Voraussetzungen können Sie das Tutorial mit einem der folgenden Tools/SDKs ausführen: Azure-Portal, Visual Studio, PowerShell, Resource Manager-Vorlage, REST-API. Wählen Sie eine der Optionen in der Dropdownliste am Anfang oder die Links am Ende dieses Artikels, um das Tutorial mithilfe einer der folgenden Optionen auszuführen.    

## <a name="tutorial-overview"></a>Übersicht über das Tutorial
In diesem Tutorial führen Sie die folgenden Schritte aus:

1. Eine **Data Factory**. Eine Data Factory kann mindestens eine Datenpipeline enthalten, die Daten verschiebt und verarbeitet. 

    In diesem Tutorial erstellen Sie eine Pipeline in der Data Factory. 
2. Erstellen einer **Pipeline** Eine Pipeline kann eine oder mehrere Aktivitäten aufweisen (Beispiele: Kopieraktivität, HDInsight Hive-Aktivität). Dieses Beispiel verwendet die HDInsight-Hive-Aktivität, die ein Hive-Skript für einen HDInsight-Hadoop-Cluster ausführt. Das Skript erstellt zunächst eine Tabelle, die auf die Webprotokoll-Rohdaten im Azure-Blobspeicher verweist, und partitioniert die Rohdaten dann nach Jahr und Monat.

    In diesem Tutorial verwendet die Pipeline die Hive-Aktivität zum Transformieren von Daten durch Ausführen einer Hive-Abfrage in einem Azure HDInsight Hadoop-Cluster. 
3. Erstellen von **verknüpften Diensten**. Sie erstellen einen verknüpften Dienst, um einen Datenspeicher oder einen Computedienst mit der Data Factory zu verknüpfen. Ein Datenspeicher wie Azure Storage hält Ein-/Ausgabedaten von Aktivitäten in der Pipeline. Ein Computedienst (etwa ein HDInsight-Hadoop-Cluster) verarbeitet/transformiert Daten.

    In diesem Tutorial erstellen Sie zwei verknüpfte Dienste: **Azure Storage** und **Azure HDInsight**. Der mit Azure Storage verknüpfte Dienst verknüpft ein Azure Storage-Konto, das die Eingabe-/Ausgabedaten enthält, mit der Data Factory. Der mit Azure HDInsight verknüpfte Dienst verknüpft einen Azure HDInsight-Cluster, der zum Transformieren von Daten verwendet wird, mit der Data Factory. 
3. Erstellen von **Datasets**für Eingabe und Ausgabe Ein Eingabedataset stellt die Eingabe für eine Aktivität in der Pipeline dar, und ein Ausgabedataset stellt die Ausgabe für die Aktivität dar.

    In diesem Tutorial legen die Eingabe- und Ausgabedatasets Speicherorte der Eingabe- und Ausgabedaten in Azure Blob Storage fest. Der mit Azure Storage verknüpfte Dienst gibt an, welches Azure Storage-Konto verwendet werden soll. Ein Eingabedataset gibt an, wo sich die Eingabedateien befinden, ein Ausgabedataset gibt an, wo die Ausgabedateien platziert werden. 


Eine ausführliche Übersicht über Azure Data Factory finden Sie im Artikel [Einführung in Azure Data Factory](data-factory-introduction.md).
  
Die Diagrammansicht der Beispiel-Data Factory, die Sie in diesem Tutorial erstellen, finden Sie **hier** . **MyFirstPipeline** weist eine Aktivität vom Typ „Hive“ auf, die das **AzureBlobInput**-Dataset als Eingabe verbraucht und das **AzureBlobOutput**-Dataset als Ausgabe erstellt. 

![Diagrammansicht im Data Factory-Tutorial](media/data-factory-build-your-first-pipeline/data-factory-tutorial-diagram-view.png)


In diesem Tutorial enthält der Ordner **inputdata** des Azure-Blobcontainers **adfgetstarted** eine Datei namens „input.log“. Diese Protokolldatei enthält Einträge von drei Monaten: Januar, Februar und März 2016. Hier sind die Beispielzeilen für jeden Monat in der Eingabedatei. 

```
2016-01-01,02:01:09,SAMPLEWEBSITE,GET,/blogposts/mvc4/step2.png,X-ARR-LOG-ID=2ec4b8ad-3cf0-4442-93ab-837317ece6a1,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,53175,871 
2016-02-01,02:01:10,SAMPLEWEBSITE,GET,/blogposts/mvc4/step7.png,X-ARR-LOG-ID=d7472a26-431a-4a4d-99eb-c7b4fda2cf4c,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,30184,871
2016-03-01,02:01:10,SAMPLEWEBSITE,GET,/blogposts/mvc4/step7.png,X-ARR-LOG-ID=d7472a26-431a-4a4d-99eb-c7b4fda2cf4c,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,30184,871
```

Wenn die Datei von der Pipeline mit HDInsight-Hive-Aktivität verarbeitet wird, führt die Aktivität ein Hive-Skript auf dem HDInsight-Cluster aus, das Eingabedaten nach Jahr und Monat partitioniert. Das Skript erstellt drei Ausgabeordner, die eine Datei mit Einträgen aus jedem Monat enthalten.  

```
adfgetstarted/partitioneddata/year=2016/month=1/000000_0
adfgetstarted/partitioneddata/year=2016/month=2/000000_0
adfgetstarted/partitioneddata/year=2016/month=3/000000_0
```

Von den obigen Beispielzeilen wird die erste (mit „2016-01-01“) in die Datei „000000_0“ im Ordner „month=1“ geschrieben. Auf ähnliche Weise wird die zweite Zeile in die Datei im Ordner „month=2“ geschrieben und die dritte Zeile in die Datei im Ordner „month=3“.  

## <a name="prerequisites"></a>Voraussetzungen
Bevor Sie mit diesem Tutorial beginnen, müssen folgende Voraussetzungen erfüllt sein:

1. **Azure-Abonnement** – Wenn Sie über kein Azure-Abonnement verfügen, können Sie in wenigen Minuten ein kostenloses Testversionskonto einrichten. Im Artikel [Kostenlose Testversion](https://azure.microsoft.com/pricing/free-trial/) erfahren Sie, wie Sie ein kostenloses Testkonto erhalten.
2. **Azure Storage** – Sie benötigen ein allgemeines Azure Storage-Standardkonto zum Speichern der Daten in diesem Tutorial. Falls Sie noch kein allgemeines Azure Storage-Standardkonto haben, lesen Sie den Artikel [Erstellen eines Speicherkontos](../storage/common/storage-create-storage-account.md#create-a-storage-account). Notieren Sie sich nach Erstellung des Speicherkontos den **Kontonamen** und den **Zugriffsschlüssel**. Siehe [Anzeigen, Kopieren und erneutes Generieren von Speicherzugriffsschlüsseln](../storage/common/storage-create-storage-account.md#view-and-copy-storage-access-keys).
3. Laden Sie die Hive-Abfragedatei (**HQL**) herunter, und sehen Sie sie durch. Sie finden sie unter [https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql](https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql). Diese Abfrage transformiert die Eingabedaten in Ausgabedaten. 
4. Laden Sie die Beispieleingabedatei (**input.log**) herunter, und sehen Sie sie durch: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log](https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log)
5. Erstellen Sie einen Blobcontainer namens **adfgetstarted** in Azure Blob Storage. 
6. Laden Sie im Container **adfgetstarted** die Datei **partitionweblogs.hql** in den Ordner **script** hoch. Verwenden Sie Tools wie etwa den [Microsoft Azure-Speicher-Explorer](http://storageexplorer.com/). 
7. Laden Sie im Container **adfgetstarted** die Datei **input.log** in den Ordner **inputdata** hoch. 

Nach dem Erfüllen der Voraussetzungen wählen Sie eines der folgenden Tools/SDKs aus, um das Tutorial auszuführen: 

- [Azure-Portal](data-factory-build-your-first-pipeline-using-editor.md)
- [Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)
- [PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)
- [Resource Manager: Vorlage](data-factory-build-your-first-pipeline-using-arm.md)
- [REST-API](data-factory-build-your-first-pipeline-using-rest-api.md)

Azure-Portal und Visual Studio bieten eine GUI-Methode zum Erstellen von Data Factorys. Die Optionen PowerShell, Resource Manager-Vorlage und REST-API bieten jedoch Skripting-/Programmiermethoden zum Erstellen von Data Factorys.

> [!NOTE]
> Die Datenpipeline in diesem Tutorial transformiert Eingabedaten in Ausgabedaten. Sie kopiert keine Daten aus einem Quelldatenspeicher in einen Zieldatenspeicher. Ein Tutorial zum Kopieren von Daten mithilfe von Azure Data Factory finden Sie unter [Kopieren von Daten aus Blob Storage in SQL-Datenbank mithilfe von Data Factory](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).
> 
> Sie können zwei Aktivitäten verketten (nacheinander ausführen), indem Sie das Ausgabedataset einer Aktivität als Eingabedataset der anderen Aktivität festlegen. Ausführliche Informationen finden Sie unter [Data Factory – Planung und Ausführung](data-factory-scheduling-and-execution.md). 





  

